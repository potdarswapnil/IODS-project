#4  Clustering and classification

##4.1. Boston dataset from MASS package

```{r}
#Load the Rquired (usual) libraries
library(MASS)
library(reshape2)
library(ggplot2)
library(GGally)
library(dplyr)
library(corrplot)
library(tidyverse)
#load the Boston Data
data("Boston")
```

##4.2. Boston Dataset

Load Boston data from the MASS package in R. The data is described in R documentation  [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html). This is related to real estate details from Boston suburban area. 

* 506 observations of 14 different parameters.
* It contains following columns
  **crim : per capita crime rate by town.
  **zn: proportion of residential land zoned for lots over 25,000 sq.ft.
  **indus: proportion of non-retail business acres per town.
  **chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
  **nox: nitrogen oxides concentration (parts per 10 million).
  **rm: average number of rooms per dwelling.
  **age:proportion of owner-occupied units built prior to 1940.
  **dis:weighted mean of distances to five Boston employment centres.
  **rad:index of accessibility to radial highways.
  **tax:full-value property-tax rate per \$10,000.
  **ptratio: pupil-teacher ratio by town.
  **black: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
  **lstat: lower status of the population (percent).
  **medv: median value of owner-occupied homes in \$1000s.


We will focus on the Crime rate and look at the data available.
```{r}
str(Boston)
summary(Boston)
glimpse(Boston)
```
##4.3. Boston Dataset : Graphical Overview
pairs(Boston)

ggpairs(Boston, diag=list(continuous="density", discrete="bar"), axisLabels="show")

```

- numerical values of corrrelation matrix: 
  
```{r}
cor_matrix<-cor(Boston) %>% round(digits =2)
cor_matrix
```
- Graphical Representation of Correlation Plots in matrix format.

```{r}
library(corrplot)
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```
In this upper triangular correlation plot, size and color of the "circle" represent the correlation value.
We can find following 

* Positive correalation of Crime rate and Rad(Distance from Highways)and Property Tax 
* Negative correlation of crime rate and Distance to Employment Centres.
* Negative correlation of crime rate and Rooms in the house
* RM and Age are normally distributed

##4.4 Standardization of data

In the scaling we subtract the column means from the corresponding columns and divide the difference with standard deviation.

**scaled(x)=x???mean(x)sd(x)

We can use the scale function to scale Boston Data set


```{r}
boston_scaled <- scale(Boston, center = TRUE, scale = TRUE)
summary(boston_scaled)
```
*Scaling helps in comparing values in different ranges.
*We can see that all the columns are now in the same scale and Mean is zero for all the columns.


##4.4. Linear Discriminant Analysis
*Linear Discriminant analysis is a classification (and dimension reduction) method. It finds the (linear) combination of the variables that separate the target variable classes. The target can be binary or multiclass variable.

* We first convert the variable into a categorical variable with values "low", "med_low", "med_high", "high". We define this vector using quantile() function, executed on scaled continues variable crime. 



```{r}
boston_scaled <- scale(Boston, center = TRUE, scale = TRUE)
class(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
scaled_crim <- boston_scaled$crim  
summary(scaled_crim)
bins <- quantile(scaled_crim) 
bins
crime <- cut(scaled_crim, breaks = bins, labels = c("low", "med_low", "med_high", "high"), include.lowest = TRUE)

```

*We look the table of categorical variable to see how many observations are in every class: 
  
  ```{r}
table(crime)
```
Next we drop the old crime rate variable from the dataset and add the new categorical value to scaled data: 
  
  ```{r}
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
```

*We divide the data to training sets and test sets with a share of 80%.
*We choose randomly 80% of the rows for the training set, and rest of the data for testing. 

```{r}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

##4.5 LDA modeling using training data

Linear discriminant model on training data 
  
```{r}
lda.fit <- lda(crime ~ ., data = train)
lda.fit
```

*Next we visualize results by using biplot function. 
*The argument *dimen* in **plot()** function determines how many discriminants are used. 
```{r}
lda.fit <- lda(crime ~ ., data = train)

lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

classes <- as.numeric(train$crime)

plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

```

##4.6 LDA for prediction: use of test data. 

*Prediction of the LDA model to classify the test data.
*In order to analyze the model performance we use cross tabulation and create a table consisting of correct and predicted classes. 

```{r}
lda.pred <- predict(lda.fit, newdata = test)
table(correct = test$crime, predicted = lda.pred$class)
```
We can use table() function to produce a matrix in order to determine how many observations were correctly or incorrectly classified. 

##4.7 Distance measures and clustering. K-means algorithm

* In clustering the number of classes is unknown, and the data are grouped based on their similarity. 
* Standardize Boston Data Boston data. 
* Eucledian distance in the dist() function creates a distance matrix consisting of pairwise distances of the observations.  

```{r}
dist_eu <- dist(boston_scaled, method = "euclidean")
summary(dist_eu)
```

K-means kmeans()is an unsupervised method, that assigns observations to groups or clusters based on similarity of the objects. It takes as an argument the number of clusters to look:
  
  ```{r}
km <-kmeans(dist_eu, centers = 4)

```

One way to determine the number of clusters is to look at how the total of within cluster sum of squares (WCSS) behaves when the number of cluster changes. The optimal number of clusters is achieved when WCSS drops significantly.  

The total within sum of squares twcss is defined below. K-means randomly assigns the initial cluster centers using the function set.seed() which might generate minor differences in the plots.

```{r}
set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
plot(1:k_max, twcss, type='b')
km <-kmeans(dist_eu, centers = 2)
pairs(Boston, col = km$cluster)
```

The results show that twcss drops significantly at  k=2. On the last graphics we calculate and plot **kmeans** function for 2 clusters because it is easier to show. 

##4.Bonus.1

```{r}
set.seed(123)
data("Boston")
boston_scaled2 <- as.data.frame(scale(Boston, center = TRUE, scale = TRUE))
dist_eu2 <- dist(boston_scaled2, method = "euclidean")
km2 <- kmeans(dist_eu2, centers = 5)
boston_scaled2$cluster <- km2$cluster
lda.fit2 <- lda(cluster ~ ., data =boston_scaled2)
plot(lda.fit2, col=as.numeric(boston_scaled2$cluster), dimen=2)
lda.arrows(lda.fit2, myscale = 3, col = "#666666")

```

##4.Bonus.2
```{r}
library(plotly)
model_predictors <- dplyr::select(train, -crime)

dim(model_predictors)
dim(lda.fit$scaling)

matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=train$crime)

train$cluster <- boston_scaled2$cluster[match(rownames(train), rownames(boston_scaled2))]
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=as.factor(train$cluster))

```
